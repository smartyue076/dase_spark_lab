#!/usr/bin/env python3
"""
Spark RDD Join å®éªŒè„šæœ¬
æ”¯æŒåˆ†åŒºå™¨é€‰æ‹©ï¼š
- hash: HashPartitioner
- range: RangePartitionerï¼ˆéœ€è¦ key å¯æ’åºï¼‰
- custom: è‡ªå®šä¹‰åˆ†åŒºå™¨ï¼ˆç®€å•ç¤ºä¾‹ï¼šå¥‡å¶ key åˆ†åŒºï¼‰

è¾“å…¥ï¼š
- æ¯è¡Œ "key,value" çš„ CSV æ–‡ä»¶
è¾“å‡ºï¼š
- Join ç»“æœå†™å…¥å•ä¸ª part-00000 æ–‡ä»¶
"""

"""
/opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --executor-memory 1G \
  --executor-cores 1 \
  /opt/spark/work-dir/join_experiment.py \
  --inputA /opt/spark/work-dir/dataset/tableA.csv \
  --inputB /opt/spark/work-dir/dataset/tableB.csv \
  --output /opt/spark/work-dir/dataset/join_result \
  --partitioner hash \
  --num-partitions 8
"""


import argparse
import sys
import time
from datetime import datetime
from pyspark import SparkContext, SparkConf
from pyspark.rdd import portable_hash

# -----------------------------
# è‡ªå®šä¹‰åˆ†åŒºå™¨ç¤ºä¾‹
# -----------------------------
class CustomPartitioner:
    """ç®€å•è‡ªå®šä¹‰åˆ†åŒºå™¨ï¼šå¥‡å¶ key åˆ†åˆ°ä¸åŒåˆ†åŒº"""
    def __init__(self, num_partitions):
        self.num_partitions = num_partitions

    def __call__(self, key):
        try:
            return int(key) % self.num_partitions
        except Exception:
            return 0  # å‡ºé”™ key æ”¾åˆ° 0 åˆ†åŒº

    def numPartitions(self):
        return self.num_partitions

# -----------------------------
# å·¥å…·å‡½æ•°
# -----------------------------
def parse_kv(line):
    try:
        key, value = line.strip().split(",", 1)
        return key, value
    except Exception:
        return None

# -----------------------------
# ä¸»å‡½æ•°
# -----------------------------
def main():
    parser = argparse.ArgumentParser(description="Spark RDD Join å®éªŒ")
    parser.add_argument("--inputA", required=True, help="è¾“å…¥æ–‡ä»¶ A (key,value)")
    parser.add_argument("--inputB", required=True, help="è¾“å…¥æ–‡ä»¶ B (key,value)")
    parser.add_argument("--output", required=True, help="è¾“å‡ºç›®å½•")
    parser.add_argument(
        "--partitioner",
        choices=["hash", "range", "custom"],
        default="hash",
        help="åˆ†åŒºç­–ç•¥"
    )
    parser.add_argument("--num-partitions", type=int, default=8, help="åˆ†åŒºæ•°é‡")
    args = parser.parse_args()

    conf = SparkConf().setAppName(f"RDD-Join-{args.partitioner}-{datetime.now().strftime('%m%d%H%M')}")
    conf.set("spark.eventLog.enabled", "true")
    conf.set("spark.eventLog.dir", "file:///tmp/spark-events")
    sc = SparkContext(conf=conf)
    sc.setLogLevel("WARN")

    print(f"ğŸ“‚ è¾“å…¥A: {args.inputA}")
    print(f"ğŸ“‚ è¾“å…¥B: {args.inputB}")
    print(f"ğŸ§© åˆ†åŒºå™¨: {args.partitioner}, åˆ†åŒºæ•°={args.num_partitions}")

    start_time = time.time()
    try:
        # è¯»å– CSV
        rddA = sc.textFile(args.inputA).map(parse_kv).filter(lambda x: x is not None)
        rddB = sc.textFile(args.inputB).map(parse_kv).filter(lambda x: x is not None)

        if args.partitioner == "hash":
            print("ğŸ”€ ä½¿ç”¨ HashPartitioner")
            rddA = rddA.partitionBy(args.num_partitions)
            rddB = rddB.partitionBy(args.num_partitions)

        elif args.partitioner == "range":
            print("ğŸ”„ ä½¿ç”¨ RangePartitioner (é€šè¿‡ sortByKey å®ç°)")
            rddA = rddA.sortByKey(ascending=True, numPartitions=args.num_partitions)
            rddB = rddB.sortByKey(ascending=True, numPartitions=args.num_partitions)

        elif args.partitioner == "custom":
            print("âœ¨ ä½¿ç”¨è‡ªå®šä¹‰åˆ†åŒºå™¨")
            part = CustomPartitioner(args.num_partitions)
            rddA = rddA.partitionBy(args.num_partitions, partitionFunc=part)
            rddB = rddB.partitionBy(args.num_partitions, partitionFunc=part)

        else:
            raise ValueError(f"æœªçŸ¥åˆ†åŒºå™¨: {args.partitioner}")

        # æ‰§è¡Œ Join
        print("â³ æ‰§è¡Œ Join ...")
        joined = rddA.join(rddB)

        # å†™å…¥ç»“æœåˆ°å•ä¸ªæ–‡ä»¶
        output_path = f"{args.output}-{int(time.time())}"
        joined.coalesce(1).saveAsTextFile(output_path)

        elapsed = time.time() - start_time
        print(f"âœ… Join å®Œæˆ! è¾“å‡º: {output_path}/part-00000")
        print(f"â± æ€»è€—æ—¶: {elapsed:.2f}s")
        print(f"â™»ï¸ æ•°æ®é‡: {joined.count()} æ¡è®°å½•")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        raise
    finally:
        sc.stop()

if __name__ == "__main__":
    main()
