#!/usr/bin/env python3
"""
ä½¿ç”¨çº¯ RDD å¯¹æœ¬åœ°æ•´æ•°æ–‡ä»¶æ’åºï¼Œæ”¯æŒé€‰æ‹©åˆ†åŒºå™¨ï¼š
- range: å…¨å±€æ’åºï¼ˆä½¿ç”¨ RangePartitionerï¼‰
- hash: å“ˆå¸Œåˆ†åŒº + åˆ†åŒºå†…æ’åºï¼ˆä½¿ç”¨ HashPartitionerï¼‰

è¾“å…¥ï¼šæ¯è¡Œä¸€ä¸ªæ•´æ•°çš„æ–‡æœ¬æ–‡ä»¶  
è¾“å‡ºï¼šæ’åºåçš„æ•´æ•°ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼Œå†™å…¥å•ä¸ª part-00000 æ–‡ä»¶

ç”¨æ³•:
/opt/spark/bin/spark-submit \
  --master spark://172.23.166.104:7077 \
  --executor-memory 1G \
  --executor-cores 1 \
  --conf spark.eventLog.enabled=true \
  --conf spark.eventLog.dir=file:///tmp/spark-events \
  /opt/spark/work-dir/code/src/sort.py \
  --input /opt/spark/work-dir/code/dataset/inbalance.txt \
  --output /opt/spark/work-dir/code/dataset/sorted_numbers \
  --partitioner hash
"""

import argparse
import sys
from datetime import datetime
from pyspark import SparkContext, SparkConf


def parse_int(s):
    try:
        return int(s.strip())
    except Exception:
        return None


def sort_partition(iterator):
    """å¯¹å•ä¸ªåˆ†åŒºå†…çš„æ•°æ®æ’åº"""
    data = list(iterator)
    data.sort()
    return iter(data)


def main():
    parser = argparse.ArgumentParser(description="RDD æ’åºï¼šæ”¯æŒ hash æˆ– range åˆ†åŒºå™¨")
    parser.add_argument("--input", required=True, help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªæ•´æ•°ï¼‰")
    parser.add_argument("--output", required=True, help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument(
        "--partitioner",
        choices=["range", "hash"],
        default="range",
        help="åˆ†åŒºç­–ç•¥: 'range'ï¼ˆå…¨å±€æ’åºï¼‰æˆ– 'hash'ï¼ˆåˆ†åŒºå†…æ’åºï¼‰"
    )
    parser.add_argument(
        "--num-partitions",
        type=int,
        default=8,
        help="åˆ†åŒºæ•°é‡"
    )
    args = parser.parse_args()

    input_path = args.input
    output_path = args.output
    partitioner_type = args.partitioner
    num_partitions = args.num_partitions

    # é…ç½® Spark
    current_time_str = datetime.now().strftime("%m%d%H%M")
    conf = SparkConf().setAppName(f"RDD-Sort-{partitioner_type}-{current_time_str}")
    # å¯ç”¨äº‹ä»¶æ—¥å¿—ï¼ˆå…¼å®¹ä½ çš„ History Serverï¼‰
    conf.set("spark.eventLog.enabled", "true")
    conf.set("spark.eventLog.dir", "file:///tmp/spark-events")
    conf.set("spark.sql.shuffle.partitions", str(num_partitions))  # è™½ç„¶ä¸ç”¨ SQLï¼Œä½†å½±å“ shuffle é»˜è®¤å€¼

    sc = SparkContext(conf=conf)
    sc.setLogLevel("WARN")

    print(f"ğŸ“‚ è¾“å…¥: {input_path}")
    print(f"ğŸ’¾ è¾“å‡º: {output_path}")
    print(f"ğŸ§© åˆ†åŒºå™¨: {partitioner_type} (partitions={num_partitions})")

    try:
        # 1. è¯»å–æ–‡æœ¬å¹¶è½¬ä¸ºæ•´æ•° RDDï¼Œè¿‡æ»¤æ— æ•ˆè¡Œ
        lines = sc.textFile(input_path)
        numbers = lines.map(parse_int).filter(lambda x: x is not None)

        if partitioner_type == "range":
            # === å…¨å±€æ’åºï¼šä½¿ç”¨ sortBy() â†’ è‡ªåŠ¨ç”¨ RangePartitioner ===
            print("ğŸ”„ æ‰§è¡Œå…¨å±€æ’åºï¼ˆRangePartitionerï¼‰...")
            sorted_rdd = numbers.sortBy(lambda x: x, ascending=True, numPartitions=num_partitions)

        elif partitioner_type == "hash":
            # === å“ˆå¸Œåˆ†åŒº + åˆ†åŒºå†…æ’åº ===
            print("ğŸ”€ æ‰§è¡Œå“ˆå¸Œåˆ†åŒº + åˆ†åŒºå†…æ’åºï¼ˆHashPartitionerï¼‰...")

            # è½¬ä¸º (key, value) å½¢å¼ä»¥ä¾¿ partitionBy
            keyed_rdd = numbers.map(lambda x: (x, x))

            # ä½¿ç”¨ HashPartitioner é‡åˆ†åŒº
            repartitioned = keyed_rdd.partitionBy(num_partitions)

            # æå– value å¹¶åœ¨æ¯ä¸ªåˆ†åŒºå†…æ’åº
            values_only = repartitioned.map(lambda kv: kv[1])
            sorted_rdd = values_only.mapPartitions(sort_partition)

        else:
            raise ValueError(f"æœªçŸ¥åˆ†åŒºå™¨: {partitioner_type}")

        # 3. å†™å…¥ç»“æœï¼ˆå¼ºåˆ¶åˆå¹¶ä¸ºå•ä¸ªæ–‡ä»¶ï¼‰
        print("â³ å†™å…¥ç»“æœ...")
        output_path = f"{output_path}-{current_time_str}"
        sorted_rdd.coalesce(1).saveAsTextFile(output_path)

        print(f"âœ… å®Œæˆï¼ç»“æœ: {output_path}/part-00000")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        raise
    finally:
        sc.stop()


if __name__ == "__main__":
    main()