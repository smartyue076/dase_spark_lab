#!/usr/bin/env python3
"""
ä½¿ç”¨çº¯ RDD å¯¹æœ¬åœ°æ•´æ•°æ–‡ä»¶æ’åºï¼Œæ”¯æŒé€‰æ‹©åˆ†åŒºå™¨ï¼š
- range: å…¨å±€æ’åºï¼ˆä½¿ç”¨ RangePartitionerï¼‰
- hash: å“ˆå¸Œåˆ†åŒº + åˆ†åŒºå†…æ’åºï¼Œä½†æœ€ç»ˆè¾“å‡ºå…¨å±€æœ‰åº

è¾“å…¥ï¼šæ¯è¡Œä¸€ä¸ªæ•´æ•°çš„æ–‡æœ¬æ–‡ä»¶  
è¾“å‡ºï¼šæ’åºåçš„æ•´æ•°ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼Œå†™å…¥å•ä¸ª part-00000 æ–‡ä»¶

ç”¨æ³•:
/opt/spark/bin/spark-submit \
  --master spark://172.23.166.104:7077 \
  --executor-memory 1G \
  --executor-cores 1 \
  --conf spark.eventLog.enabled=true \
  --conf spark.eventLog.dir=file:///tmp/spark-events \
  /opt/spark/work-dir/xy/dase_spark_lab/code/src/sort.py \
  --input /opt/spark/work-dir/xy/dase_spark_lab/code/dataset/heavy_skew.txt \
  --output /opt/spark/work-dir/xy/dase_spark_lab/code/dataset/sorted_numbers \
  --partitioner range
"""

import argparse
import sys
import os
from datetime import datetime
from pyspark import SparkContext, SparkConf
from pyspark.rdd import portable_hash

def parse_key(s):
    v = s.strip()
    return v if v else None

def sort_partition(iterator):
    data = list(iterator)
    data.sort()
    return iter(data)

def main():
    parser = argparse.ArgumentParser(description="RDD æ’åºå¹¶å¯¼å‡ºæ¯åˆ†åŒºæ•°æ®")
    parser.add_argument("--input", required=True, help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªæ•´æ•°ï¼‰")
    parser.add_argument("--output", required=True, help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument(
        "--partitioner",
        choices=["range", "hash"],
        default="range",
        help="åˆ†åŒºç­–ç•¥: range æˆ– hash"
    )
    parser.add_argument(
        "--num-partitions",
        type=int,
        default=4,
        help="åˆ†åŒºæ•°é‡"
    )
    args = parser.parse_args()

    input_path = args.input
    output_path = args.output
    partitioner_type = args.partitioner
    num_partitions = args.num_partitions

    # é…ç½® Spark
    current_time_str = datetime.now().strftime("%m%d%H%M")
    filename_with_ext = os.path.basename(input_path)
    filename = os.path.splitext(filename_with_ext)[0]
    conf = SparkConf().setAppName(f"A-Sort-{partitioner_type}-{filename}-{current_time_str}")
    conf.set("spark.eventLog.enabled", "true")
    conf.set("spark.eventLog.dir", "file:///tmp/spark-events")
    conf.set("spark.sql.shuffle.partitions", str(num_partitions))

    output_path = f"{output_path}-{current_time_str}"
    partition_dump_path = f"{output_path}-partitions"

    sc = SparkContext(conf=conf)
    sc.setLogLevel("WARN")

    print(f"ğŸ“‚ è¾“å…¥: {input_path}")
    print(f"ğŸ’¾ è¾“å‡º: {output_path}")
    print(f"ğŸ“ åˆ†åŒºæ•°æ® dump: {partition_dump_path}")
    print(f"ğŸ§© åˆ†åŒºå™¨: {partitioner_type} (numPartitions={num_partitions})")

    try:
        # 1. åŠ è½½æ•°æ®
        lines = sc.textFile(input_path)
        numbers = lines.map(parse_key).filter(lambda x: x is not None)

        if partitioner_type == "range":
            print("ğŸ”„ æ‰§è¡Œå…¨å±€æ’åºï¼ˆRangePartitionerï¼‰...")
            sorted_rdd = numbers.sortBy(lambda x: x, ascending=True, numPartitions=num_partitions)

        elif partitioner_type == "hash":
            print("ğŸ”€ æ‰§è¡Œå“ˆå¸Œåˆ†åŒº + åˆ†åŒºå†…æ’åº + å…¨å±€æ’åº...")
            keyed_rdd = numbers.map(lambda x: (x, x))
            repartitioned = keyed_rdd.partitionBy(num_partitions, partitionFunc=portable_hash)
            locally_sorted = repartitioned.map(lambda kv: kv[1]).mapPartitions(sort_partition)
            sorted_rdd = locally_sorted.sortBy(lambda x: x, ascending=True, numPartitions=num_partitions)

        else:
            raise ValueError(f"æœªçŸ¥åˆ†åŒºå™¨ç±»å‹: {partitioner_type}")

        # # 2. ä¿å­˜æ¯åˆ†åŒºçš„æ•°æ®ï¼ˆå…³é”®éƒ¨åˆ†ï¼ï¼‰
        # print("ğŸ“ ä¿å­˜æ¯ä¸ªåˆ†åŒºçš„æ•°æ®...")
        # partition_dump_rdd = sorted_rdd.mapPartitionsWithIndex(
        #     lambda it: (f"{v}" for v in it)
        # )
        # partition_dump_rdd.saveAsTextFile(partition_dump_path)

        # 3. ä¿å­˜æœ€ç»ˆæ’åºç»“æœ
        print("â³ å†™å…¥æœ€ç»ˆæ’åºè¾“å‡º...")
        sorted_rdd.coalesce(1).saveAsTextFile(output_path)

        print(f"âœ… å®Œæˆï¼ç»“æœï¼š{output_path}/part-00000")
        # print(f"ğŸ“ åˆ†åŒº dumpï¼š{partition_dump_path}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        raise
    finally:
        sc.stop()

if __name__ == "__main__":
    main()